{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f505cb",
   "metadata": {},
   "source": [
    "## 一. 数据来源\n",
    "一般进行NLP的数据来源可能有以下几种。\n",
    "### 1. 外部文本文件\n",
    "文件格式可能是.txt,.json或者.csv，或者某种其他约定好格式的文件。这种需要按照相应的格式读入，存入可以处理的数据结构。Python或者Numpy的数组，或者Pandas的DataFrame。  \n",
    "\n",
    "### 2. 外部数据文件\n",
    "有些数据可能来自于现有数据库，需要读取并且转换。\n",
    "\n",
    "### 3. 网络抓取\n",
    "从网络抓取数据（文本）进行处理也是现在很流行的一种方法。Python的网络爬虫开发和抓取的文本处理功能非常强大。非常方便对于网上的新闻，文本和各种社交网站信息进行抓取和分析。   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7eb7d",
   "metadata": {},
   "source": [
    "## 二. 数据处理（数据清洗）\n",
    "拿到需要的数据以后，根据数据的情况，需要对数据进行预处理。比如对格式进行统一，去掉与计算和处理无关的内容，只保留有意义的内容。\n",
    "常见的处理包括：\n",
    "1、删除不需要处理的特殊符号如■、©等\n",
    "2、去除字符见多余的空格、删除连续出现的标点符号，删除不出现中文字符的数据行\n",
    "3、统一数据格式，比如英文是不是需要统一大小写（对于大小写无关的任务），中文是不是要统一编码格式（GBK还是Utf8）\n",
    "4、分析数据内容，拆分不同区域。\n",
    "例如：一条Twitter信息，\n",
    "PragerU@prageru NOW STREAMING: PragerU’s short documentary, #DearInfidels: A Warning to America.\n",
    "其中@后面跟的是用户的Twitter账号，#后面跟的是这条内容的标签。中间的其他部分才是内容信息。正常这类信息的区分可以由抓取评论的程序处理。但是有时候为了给后续的处理提供最大的信息量（事先可能不确定后续程序是否需要这些账号和标签信息），也可能选择把这些信息送给后续程序有后续处理程序选择是否需要和如何处理。所以我们的数据处理部分也需要根据我们的模型需求决定是不是要处理这类数据。比如我们的模型如果只是完成对于评论内容进行分析的，我们的预处理程序就需要删除账号和标签数据才能让模型更准确。  \n",
    "\n",
    "这类处理主要是字符串插入，删除，修改等操作。一般简单的可以通过Python的字符串处理功能完成。如果数据量比较大，情况复杂，正则表达式是进行字符串操作的强力工具。\n",
    "\n",
    "\n",
    "### 1. Python的字符串处理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0615df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "字符串长度: 13\n",
      "第一个字符: H\n",
      "修改后的字符串: hello, world!\n",
      "子字符串 'world' 的位置: 7\n",
      "子字符串 'world' 的位置: 7\n",
      "子字符串 'world' 的位置: 20\n",
      "子字符串 'world' 的位置: -1\n",
      "替换后的字符串: Hello, Python! Hello Python!  Hello Python!  Hello Python!\n",
      "Hello, Python! Hello Python!  Hello world!  Hello world!\n",
      "Hello, world! Hello Python!  Hello world!  Hello world!\n",
      "分割后的列表: ['Hello', 'world!']\n",
      "全部大写: HELLO, WORLD!\n",
      "全部小写: hello, world!\n",
      "每个单词首字母大写: Hello, World!\n",
      "只有第一个单词首字母大写: Hello, world!\n",
      "检查是否以 'ex' 开头: True\n",
      "检查是否以 '.txt' 结尾: True\n",
      "连接后的字符串: Hello world from Python\n"
     ]
    }
   ],
   "source": [
    "# 统计字符串长度\n",
    "s = \"Hello, world!\"\n",
    "length = len(s)\n",
    "print(\"字符串长度:\", length)\n",
    "\n",
    "# 访问和修改字符串\n",
    "s = \"Hello, world!\"\n",
    "# 访问第一个字符\n",
    "first_char = s[0]\n",
    "print(\"第一个字符:\", first_char)\n",
    "\n",
    "# 尝试修改第一个字符（错误示例，将被注释）\n",
    "# s[0] = 'h'  # 这将引发错误\n",
    "\n",
    "# 使用切片和连接创建新字符串\n",
    "new_s = 'h' + s[1:]\n",
    "print(\"修改后的字符串:\", new_s)\n",
    "\n",
    "# 查找字符串\n",
    "s = \"Hello, world! Hello world!\"\n",
    "position = s.find(\"world\")\n",
    "print(\"子字符串 'world' 的位置:\", position)\n",
    "\n",
    "# 从刚刚找到的位置开始继续找，找到的还是刚刚这个\n",
    "position = s.find(\"world\", position)\n",
    "print(\"子字符串 'world' 的位置:\", position)\n",
    "\n",
    "# 从刚刚找到的位置下一个开始继续找，找到的是下一个\n",
    "position = s.find(\"world\", position+1)\n",
    "print(\"子字符串 'world' 的位置:\", position)\n",
    "\n",
    "# 找不到返回-1\n",
    "position = s.find(\"world\", position+1)\n",
    "print(\"子字符串 'world' 的位置:\", position)\n",
    "\n",
    "# 替换字符串，注意使用replace一次会替换所有满足条件的字符串\n",
    "s = \"Hello, world! Hello world!  Hello world!  Hello world!\"\n",
    "new_s = s.replace(\"world\", \"Python\")\n",
    "print(\"替换后的字符串:\", new_s)\n",
    "\n",
    "# replace函数可以带一个count参数，表示替换前n个，\n",
    "s = \"Hello, world! Hello world!  Hello world!  Hello world!\"\n",
    "new_s = s.replace(\"world\", \"Python\",2)\n",
    "print(new_s)\n",
    "\n",
    "# 对于需要跳过前面只替换后面的字符串的情况，需要手工查找并且切片拼接形成新的字符串\n",
    "s = \"Hello, world! Hello world!  Hello world!  Hello world!\"\n",
    "\n",
    "# 找到第二个 'foo' 的位置\n",
    "first_pos = s.find('world')  # 找到第一个 'foo'\n",
    "# 从第一个 'foo' 后开始找第二个 'foo'\n",
    "second_pos = s.find('world', first_pos + 1)\n",
    "\n",
    "# 替换第二个 'world'\n",
    "if second_pos != -1:\n",
    "    new_s = s[:second_pos] + 'Python' + s[second_pos + len('world'):]\n",
    "else:\n",
    "    new_s = s  # 如果没有找到第二个 'world'，不做改变\n",
    "\n",
    "print(new_s)\n",
    "\n",
    "# 分割字符串\n",
    "s = \"Hello, world!\"\n",
    "words = s.split(\", \")\n",
    "print(\"分割后的列表:\", words)\n",
    "\n",
    "# 字符串大小写转换\n",
    "s = \"hello, world!\"\n",
    "\n",
    "print(\"全部大写:\", s.upper())\n",
    "print(\"全部小写:\", s.lower())\n",
    "print(\"每个单词首字母大写:\", s.title())\n",
    "print(\"只有第一个单词首字母大写:\", s.capitalize())\n",
    "\n",
    "# 检查字符串开头或结尾\n",
    "s = \"example.txt\"\n",
    "\n",
    "print(\"检查是否以 'ex' 开头:\", s.startswith(\"ex\"))\n",
    "print(\"检查是否以 '.txt' 结尾:\", s.endswith(\".txt\"))\n",
    "\n",
    "\n",
    "# 连接字符串列表\n",
    "words = ['Hello', 'world', 'from', 'Python']\n",
    "s = ' '.join(words)\n",
    "print(\"连接后的字符串:\", s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143d81b9",
   "metadata": {},
   "source": [
    "### 2. 正则表达式\n",
    "正则表达式是一种强大的文本处理工具，用于在字符串中执行模式匹配、搜索和替换操作。它们通过定义一个搜索模式来识别字符串中的特定结构，这个模式可以是一系列字符、一个特定的词或者一种复杂的字符串组合。\n",
    "\n",
    "#### 基本组成\n",
    "正则表达式通常由文字字符和特殊字符（元字符）组成，每个元字符在正则表达式中都有特定的含义。以下是一些基本的元字符及其作用：  \n",
    "\n",
    ".：匹配除换行符以外的任意单个字符。  \n",
    "^：匹配输入字符串的开始位置。  \n",
    "$：匹配输入字符串的结束位置。  \n",
    "*：匹配前面的子表达式零次或多次。  \n",
    "+：匹配前面的子表达式一次或多次。  \n",
    "?：匹配前面的子表达式零次或一次。    \n",
    "\\\\：将下一个字符标记为特殊字符或字面量（例如，\\n 匹配一个换行符）。  \n",
    "[]：字符集合。匹配所包含的任意一个字符（例如，[abc] 会匹配 \"a\", \"b\", 或 \"c\"）。  \n",
    "|：选择符，匹配两项之间的任意一项（例如，a|b 会匹配 \"a\" 或 \"b\"）。  \n",
    "{}：量词，指定前面元素的重复次数（例如，a{2,3} 会匹配 \"aa\" 或 \"aaa\"）。  \n",
    "()：标记一个子表达式的开始和结束位置，并将数据保存为一个组，这可以用于提取信息或者应用重复操作。  \n",
    "\n",
    "\n",
    "#### 应用场景\n",
    "正则表达式在许多文本处理和数据提取任务中非常有用，例如：\n",
    "\n",
    "**数据验证：** 检查输入数据（如电子邮件地址、电话号码、用户ID）是否符合预期的格式。  \n",
    "**数据抽取：** 从文本中提取特定信息，如提取日期、时间、邮箱等。  \n",
    "**字符串操作：** 搜索和替换文本中的特定部分。  \n",
    "**复杂的文本分析：** 在大量文本中寻找符合复杂模式的字符串。  \n",
    "\n",
    "Python中的re模块提供了丰富的正则表达式操作。以下是一些典型的使用正则表达式进行查找和替换的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87266ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到的匹配项: ['Spain', 'Start']\n",
      "找到的邮件地址： ['info@example.com', 'tony.Ma.2006@example.org']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The rain in Spain,Start tomorrow.\"\n",
    "# 这类的 r'\\bS\\w+'就是一个正则表达式。\\b匹配一个词分隔符，可以是空格，标点符号等。S就是大写字母S，\\w表示一个词，+表示出现一次或多次。\n",
    "matches = re.findall(r'\\bS\\w+', text)\n",
    "print(\"找到的匹配项:\", matches)\n",
    "\n",
    "# 匹配符合邮件地址。前面有一个词分隔符，然后有最少一个到多个A-Za-z0-9._%+-，一个@，再来一个到多个A-Za-z0-9._%+-\n",
    "# 再来一个.，然后又是一个到多个A-Za-z0-9._%+-，最后被词分隔符分隔\n",
    "text = \"For more information, contact info@example.com or tony.Ma.2006@example.org.\"\n",
    "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', text)\n",
    "print(\"找到的邮件地址：\", emails)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18172fa",
   "metadata": {},
   "source": [
    "对于简单的文本处理，使用Python的字符串处理功能就可以完成了。但是遇到复杂的情况，编程实现就有大量的额外工作要做，如果能够合理利用正则表达式，可以大大提高编程和处理的效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07dd72a",
   "metadata": {},
   "source": [
    "## 三. 数据处理（预处理）\n",
    "### 1. 分词\n",
    "对于中文或者其他连续性文字，需要先进行分词。  \n",
    "中文的分词算法有很多，例如基于规则的算法、基于统计的算法和基于深度学习的算法。其中，基于深度学习的算法如BERT、LSTM等，能够更好地处理语义信息。一般很多实际的项目中，会直接使用现有的分词工具。下面我们给出一些现在用的比较多的分词工具的使用 ( 这里主要介绍一下中文的分词工具Jieba-结巴，英文的用的比较多的应该是NLTK和SpaCY )。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4dcff7b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jieba分词结果： 自然语言 处理 是 人工智能 领域 的 一个 重要 方向 。\n",
      "Model loaded succeed\n",
      "THULAC分词结果： 自然 语言 处理 是 人工智能 领域 的 一个 重要 方向 。\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 使用LTP分词\\nfrom ltp import LTP\\nltp = LTP()\\nseg, _ = ltp.seg([text])\\nprint(\"LTP分词结果：\", \" \".join(seg[0]))\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"自然语言处理是人工智能领域的一个重要方向。\"\n",
    " \n",
    "# 使用jieba分词\n",
    "import jieba\n",
    "seg_list = jieba.cut(text)\n",
    "print(\"jieba分词结果：\", \" \".join(seg_list))\n",
    " \n",
    "# 使用THULAC分词\n",
    "import thulac\n",
    "thu = thulac.thulac()\n",
    "seg_list = thu.cut(text)\n",
    "print(\"THULAC分词结果：\", \" \".join([item[0] for item in seg_list]))\n",
    " \n",
    "# 又是一个技术发展过快的例子，下面的HanLP不支持Python 3.8以后的版本，\n",
    "\n",
    "'''\n",
    "# 使用HanLP分词\n",
    "from pyhanlp import HanLP\n",
    "seg_list = HanLP.segment(text)\n",
    "print(\"HanLP分词结果：\", \" \".join([term.word for term in seg_list]))\n",
    "'''  \n",
    "\n",
    "# TLP已经没法下载了。\n",
    "'''\n",
    "# 使用LTP分词\n",
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "seg, _ = ltp.seg([text])\n",
    "print(\"LTP分词结果：\", \" \".join(seg[0]))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d6b52a",
   "metadata": {},
   "source": [
    "### 2. 去除停用词\n",
    "在自然语言处理（NLP）研究中，停用词stopwords是指在文本中频繁出现但通常没有太多有意义的词语。这些词语往往是一些常见的功能词、虚词甚至是一些标点符号，如介词、代词、连词、助动词等，比如中文里的\"的\"、\"是\"、\"和\"、\"了\"、\"。\"等等，英文里的\"the\"、\"is\"、\"and\"、\"...\"等等。  \n",
    "\n",
    "停用词的作用是在文本分析过程中过滤掉这些常见词语，从而减少处理的复杂度，提高算法效率，并且在某些任务中可以改善结果的质量，避免分析结果受到这些词的干扰。  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2340c986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  在 自然语言 处理 中 ， 通常 把 停 用词 、 出现 频率 很 低 词汇 过滤 掉 。 这个 过程 其实 类似 于 特征 筛选 过程 。 当然 停 用词 过滤 ， 文本 分析 中 预处理 方法 。 它 功能 过滤 分词 结果 中 噪声 。 比如 ： 、 、 啊 等 。 在 英文 里 ， 经常 会 遇到 比如 “ ” ， “ ” ， “ their ” 等 这些 可以 作为 停 用词 来 处理 ， 但是 也 考虑 自己 应用 场景 。 当然 如果 出现 一种 出现 频率 特别 低 词汇 对 分析 作用 不 大 ， 所以 一般般 也 会 去掉 。 把 停 用词 、 出现 频率 低 词 过滤 之后 ， 即 可以 得到 词典 库 。 以下 用 python 实现 停 用词 过滤 实现 ： \n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import jieba\n",
    " \n",
    "# jieba.load_userdict('userdict.txt')\n",
    "# 创建停用词list\n",
    "def stopwordslist(filepath):\n",
    "    stopwords = [line.strip() for line in open(filepath, 'r',encoding='utf-8').readlines()]\n",
    "    return stopwords\n",
    " \n",
    " \n",
    "# 对句子进行分词\n",
    "def seg_sentence(sentence):\n",
    "    sentence_seged = jieba.cut(sentence.strip())\n",
    "    stopwords = stopwordslist('JiebaStopWords.txt')  # 这里加载停用词的路径\n",
    "    outstr = ''\n",
    "    for word in sentence_seged:\n",
    "        if word not in stopwords:\n",
    "            if word != '\\t':\n",
    "                outstr += word\n",
    "                outstr += \" \"\n",
    "    return outstr\n",
    " \n",
    " \n",
    "inputs = ['the 在自然语言处理中，我们通常把停用词、出现频率很低的词汇过滤掉。这个过程其实类似于特征筛选的过程。当然停用词过滤，是文本分析中一个预处理方法。它的功能是过滤分词结果中的噪声。比如：的、是、啊等。在英文里，我们经常会遇到比如“the”，“an”，“their”等这些都可以作为停用词来处理，但是也考虑自己的应用场景。当然如果出现一种出现频率特别低的词汇对分析作用不大，所以一般般也会去掉。把停用词、出现频率低的词过滤之后，即可以得到一个我们的词典库。以下我们用python实现停用词过滤的实现：'] #加载要处理的文件的路径\n",
    "outputs = []\n",
    "for line in inputs:\n",
    "    line_seg = seg_sentence(line)  # 这里的返回值是字符串\n",
    "    outputs.append(line_seg)\n",
    "\n",
    "for line in outputs:\n",
    "    print(line)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e4fca",
   "metadata": {},
   "source": [
    "上面的例子，使用外部文件保存了停用词列表，这样可以方便地修改。（例子的停用词表很小）  \n",
    "\n",
    "对于英文文本，比较常用的文本处理库是NLTK，SpaCY，还有Gensim。  \n",
    "\n",
    "NLTK自带了停用词表（如果本地没有，会自动下载）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "85fab504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Original Sentence \n",
      "\n",
      "\n",
      "He determined to drop his litigation with the monastry , and relinguish his claims to the wood-cuting and fishery rihgts at once . He was the more ready to do this becuase the rights had become much less valuable , and he had indeed the vaguest idea where the wood and river in question were .\n",
      "\n",
      "\n",
      "Filtered Sentence \n",
      "\n",
      "\n",
      "He determined drop litigation monastry , relinguish claims wood-cuting fishery rihgts . He ready becuase rights become much less valuable , indeed vaguest idea wood river question .\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码是使用nltk从句子中去除停用词\n",
    "\n",
    "# 导入包\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "# 使用系统自带英文停用词列表\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# 例句\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "# 停用词集合\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# 分词\n",
    "word_tokens = word_tokenize(text) \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\nOriginal Sentence \\n\\n\")\n",
    "print(\" \".join(word_tokens)) \n",
    "\n",
    "print(\"\\n\\nFiltered Sentence \\n\\n\")\n",
    "print(\" \".join(filtered_sentence)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cfb22c",
   "metadata": {},
   "source": [
    "spaCy是NLP中功能最多，使用最广泛的库之一。我们可以使用SpaCy快速有效地从给定文本中删除停用词。它有一个自己的停用词列表，可以从spacy.lang.en.stop_words类导入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "64658d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始词表：\n",
      "['He', 'determined', 'to', 'drop', 'his', 'litigation', 'with', 'the', 'monastry', ',', 'and', 'relinguish', 'his', 'claims', 'to', 'the', 'wood', '-', 'cuting', 'and', '\\n', 'fishery', 'rihgts', 'at', 'once', '.', 'He', 'was', 'the', 'more', 'ready', 'to', 'do', 'this', 'becuase', 'the', 'rights', 'had', 'become', 'much', 'less', 'valuable', ',', 'and', 'he', 'had', '\\n', 'indeed', 'the', 'vaguest', 'idea', 'where', 'the', 'wood', 'and', 'river', 'in', 'question', 'were', '.']\n",
      "去除停用词：\n",
      "['determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood', '-', 'cuting', '\\n', 'fishery', 'rihgts', '.', 'ready', 'becuase', 'rights', 'valuable', ',', '\\n', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "# 加载英语分词器、标记器、解析器、NER和单词向量\n",
    "nlp = English()\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "#  \"nlp\"对象用于创建具有语言注释的文档。\n",
    "my_doc = nlp(text)\n",
    "\n",
    "# 构建词列表\n",
    "token_list = []\n",
    "for token in my_doc:\n",
    "    token_list.append(token.text)\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "# 去除停用词后创建单词列表\n",
    "filtered_sentence =[] \n",
    "\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "print('原始词表：')\n",
    "print(token_list)\n",
    "print('去除停用词：')\n",
    "print(filtered_sentence)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0375439",
   "metadata": {},
   "source": [
    "Gensim是一个非常方便的库，可以处理NLP任务。在预处理时，gensim也提供了去除停用词的方法。我们可以从类gensim.parsing.preprocessing轻松导入remove_stopwords方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9c637aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Filtered Sentence \n",
      "\n",
      "\n",
      "He determined drop litigation monastry, relinguish claims wood-cuting fishery rihgts once. He ready becuase rights valuable, vaguest idea wood river question were.\n"
     ]
    }
   ],
   "source": [
    "# 以下代码使用Gensim去除停用词\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# pass the sentence in the remove_stopwords function\n",
    "result = remove_stopwords(\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, \n",
    "and he had indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
    "\n",
    "print('\\n\\n Filtered Sentence \\n\\n')\n",
    "print(result)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af03b73d",
   "metadata": {},
   "source": [
    "### 3. 文本标准化(text normalization)\n",
    "\n",
    "在一些自然语言中（英语比较典型），根据情况，可以以多种形式书写或说出单词。这就是语言的精美之处。例如：\n",
    "\n",
    "Lisa **ate** the food and washed the dishes.  \n",
    "They were **eating** noodles at a cafe.  \n",
    "Don’t you want to **eat** before we leave?  \n",
    "We have just **eaten** our breakfast.  \n",
    "It also **eats** fruit and vegetables.  \n",
    "\n",
    "在所有这些句子中，我们可以看到\"eat\"这个词有多种形式。对我们来说，很容易理解\"eat\"就是这里具体的活动。所以对我们来说，无论是'eat'，'ate'还是'eaten'都没关系,因为我们知道发生了什么。  \n",
    "\n",
    "不幸的是，机器并非如此。他们区别对待这些词。因此，我们需要将它们标准化为它们的根词，在我们的例子中是\"eat\"。  \n",
    "\n",
    "因此，文本标准化是将单词转换为单个规范形式的过程。这可以通过两个过程来实现，即词干化(stemming)或词形还原(lemmatization)。让我们详细了解它们的含义。    \n",
    "文本标准化的另一个好处是它减少了文本数据中词典的大小。这有助于缩短机器学习模型的训练时间。  \n",
    "\n",
    "**词干化**  \n",
    "让我们先了解词干化：  \n",
    "词干化是一种文本标准化技术，它通过考虑可以在该词中找到的公共前缀或后缀列表来切断单词的结尾或开头。  \n",
    "这是一个基于规则的基本过程，从单词中删除后缀(\"ing\"，\"ly\"，\"es\"，\"s\"等)  \n",
    "**词形还原**  \n",
    "另一方面，词形还原是一种结构化的程序，用于获得单词的根形式。它利用了词汇(词汇的字典重要性程度)和形态分析(词汇结构和语法关系)。  \n",
    "\n",
    "词干化算法通过从词中剪切后缀或前缀来工作。词形还原是一种更强大的操作，因为它考虑了词的形态分析。  \n",
    "\n",
    "词形还原返回词根，词根是其所有变形形式的根词。  \n",
    "\n",
    "我们可以说词干化是一种快速但不那么好的方法，可以将词语切割成词根形式，而另一方面，词形还原是一种智能操作，它使用由深入的语言知识创建的词典。因此，词形还原有助于形成更好的效果。  \n",
    "\n",
    "**使用NLTK进行文本标准化**  \n",
    "NLTK库有许多令人惊奇的方法来执行不同的数据预处理步骤。有些方法如PorterStemmer()和WordNetLemmatizer()分别执行词干化和词形还原。  \n",
    "\n",
    "让我们看看他们的实际效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "43062de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['he', 'determin', 'drop', 'litig', 'monastri', ',', 'relinguish', 'claim', 'wood-cut', 'fisheri', 'rihgt', '.', 'he', 'readi', 'becuas', 'right', 'becom', 'much', 'less', 'valuabl', ',', 'inde', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "# 词干化\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(text) \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "Stem_words = []\n",
    "ps =PorterStemmer()\n",
    "for w in filtered_sentence:\n",
    "    rootWord=ps.stem(w)\n",
    "    Stem_words.append(rootWord)\n",
    "print(filtered_sentence)\n",
    "print(Stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f38e8a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Autin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'determined', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claims', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'rights', 'become', 'much', 'less', 'valuable', ',', 'indeed', 'vaguest', 'idea', 'wood', 'river', 'question', '.']\n",
      "['He', 'determine', 'drop', 'litigation', 'monastry', ',', 'relinguish', 'claim', 'wood-cuting', 'fishery', 'rihgts', '.', 'He', 'ready', 'becuase', 'right', 'become', 'much', 'le', 'valuable', ',', 'indeed', 'vague', 'idea', 'wood', 'river', 'question', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# 下载特定资源，例如WordNet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "text = \"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\"\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "word_tokens = word_tokenize(text) \n",
    "\n",
    "filtered_sentence = [] \n",
    "\n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "print(filtered_sentence) \n",
    "\n",
    "lemma_word = []\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "for w in filtered_sentence:\n",
    "    word1 = wordnet_lemmatizer.lemmatize(w, pos = \"n\")\n",
    "    word2 = wordnet_lemmatizer.lemmatize(word1, pos = \"v\")\n",
    "    word3 = wordnet_lemmatizer.lemmatize(word2, pos = (\"a\"))\n",
    "    lemma_word.append(word3)\n",
    "print(lemma_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ae3b4e",
   "metadata": {},
   "source": [
    "**使用spaCy进行文本标准化** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07e35963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he',\n",
       " 'determine',\n",
       " 'to',\n",
       " 'drop',\n",
       " 'his',\n",
       " 'litigation',\n",
       " 'with',\n",
       " 'the',\n",
       " 'monastry',\n",
       " ',',\n",
       " 'and',\n",
       " 'relinguish',\n",
       " 'his',\n",
       " 'claim',\n",
       " 'to',\n",
       " 'the',\n",
       " 'wood',\n",
       " '-',\n",
       " 'cut',\n",
       " 'and',\n",
       " '\\n',\n",
       " 'fishery',\n",
       " 'rihgts',\n",
       " 'at',\n",
       " 'once',\n",
       " '.',\n",
       " 'he',\n",
       " 'be',\n",
       " 'the',\n",
       " 'more',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'do',\n",
       " 'this',\n",
       " 'becuase',\n",
       " 'the',\n",
       " 'right',\n",
       " 'have',\n",
       " 'become',\n",
       " 'much',\n",
       " 'less',\n",
       " 'valuable',\n",
       " ',',\n",
       " 'and',\n",
       " 'he',\n",
       " 'have',\n",
       " '\\n',\n",
       " 'indeed',\n",
       " 'the',\n",
       " 'vague',\n",
       " 'idea',\n",
       " 'where',\n",
       " 'the',\n",
       " 'wood',\n",
       " 'and',\n",
       " 'river',\n",
       " 'in',\n",
       " 'question',\n",
       " 'be',\n",
       " '.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#确保使用\"python -m spacy download en_core_web_sm\"下载英语模型\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(u\"\"\"He determined to drop his litigation with the monastry, and relinguish his claims to the wood-cuting and \n",
    "fishery rihgts at once. He was the more ready to do this becuase the rights had become much less valuable, and he had \n",
    "indeed the vaguest idea where the wood and river in question were.\"\"\")\n",
    "\n",
    "lemma_word1 = [] \n",
    "for token in doc:\n",
    "    lemma_word1.append(token.lemma_)\n",
    "lemma_word1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a28b95",
   "metadata": {},
   "source": [
    "### 4.词性标注\n",
    "\n",
    "词性标注是指对分词后的词语进行词性标记，例如名词、动词、形容词等。词性标注能够帮助模型更好地理解文本，同时也有助于其他任务，例如命名实体识别和依存句法分析等。词性标注算法包括基于规则的算法、基于统计的算法和基于深度学习的算法。\n",
    "\n",
    "**使用Jieba对中文进行词性标注**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15365b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Autin\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.703 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "云日明 nr\n",
      "松雪 nr\n",
      "， x\n",
      "溪山 ns\n",
      "进 v\n",
      "晚风 n\n",
      "。 x\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as pseg\n",
    "\n",
    "text = \"云日明松雪，溪山进晚风。\"\n",
    "result = pseg.cut(text)\n",
    "\n",
    "for word, tag in result:\n",
    "    print(word, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3903217a",
   "metadata": {},
   "source": [
    "**使用NLTK进行词性标注**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d8ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'NNP'), ('is', 'VBZ'), ('looking', 'VBG'), ('at', 'IN'), ('buying', 'VBG'), ('U.K.', 'NNP'), ('startup', 'NN'), ('for', 'IN'), ('$', '$'), ('1', 'CD'), ('billion', 'CD')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Autin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 为了确保POS tagger正常工作，需要下载对应的数据\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# 示例文本\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# 分词\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# 词性标注\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# 输出标注结果\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d03f628",
   "metadata": {},
   "source": [
    "**使用spaCy进行词性标注**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30be3fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "1 NUM\n",
      "billion NUM\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载英文模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 示例文本\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "\n",
    "# 处理文本\n",
    "doc = nlp(text)\n",
    "\n",
    "# 输出每个词的文本和词性标签\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fdf9d",
   "metadata": {},
   "source": [
    "**NLTK** 提供了多种标注器，并且允许你训练自己的标注器或使用预训练的标注器。它适合于教学和基础研究。\n",
    "**spaCy** 提供了一个更为现代和高效的方法，适用于实际应用和大规模数据处理，因为它的模型是基于最新的深度学习算法预训练的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658dde3c",
   "metadata": {},
   "source": [
    "### 5.实体识别\n",
    "命名实体识别（Named Entity Recognition, NER）是自然语言处理中的一个重要任务，它涉及识别文本中的实体（如人名、地点、组织等）。实体识别通常采用基于规则的方法或基于机器学习的方法，例如条件随机场（CRF）和循环神经网络（RNN）等。 NLTK和spaCy都提供了实体识别的功能，但spaCy在这方面更为强大和现代化，特别是因为它的模型是基于最新的机器学习技术训练的。\n",
    "\n",
    "**使用NLTK进行命名实体识别** \n",
    "NLTK提供了一个预训练的命名实体识别器，可以用来识别文本中的各种实体。以下是如何使用NLTK进行命名实体识别的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "654c553d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Autin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Autin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Apple/NNP)\n",
      "  (ORGANIZATION CEO/NNP Tim/NNP Cook/NNP)\n",
      "  announced/VBD\n",
      "  the/DT\n",
      "  new/JJ\n",
      "  iPhone/NN\n",
      "  in/IN\n",
      "  (GPE San/NNP Francisco/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# 确保已经下载了相应的数据集\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# 示例文本\n",
    "text = \"Apple CEO Tim Cook announced the new iPhone in San Francisco.\"\n",
    "\n",
    "# 分词并进行词性标注\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "# 命名实体识别\n",
    "entities = ne_chunk(tags)\n",
    "\n",
    "# 输出实体\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23297fa8",
   "metadata": {},
   "source": [
    "**使用spaCy进行命名实体识别** \n",
    "spaCy内置了更先进的命名实体识别功能，这些功能是基于大规模语料库和神经网络模型训练得到的。以下是使用spaCy进行命名实体识别的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a142fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "Tim Cook PERSON\n",
      "iPhone ORG\n",
      "San Francisco GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载英文模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 示例文本\n",
    "text = \"Apple CEO Tim Cook announced the new iPhone in San Francisco.\"\n",
    "\n",
    "# 处理文本\n",
    "doc = nlp(text)\n",
    "\n",
    "# 输出实体及其类型\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864510b",
   "metadata": {},
   "source": [
    "NLTK：适用于教学和基础的NLP任务，但在实体识别方面可能不如spaCy准确。  \n",
    "spaCy：提供了一个现代、快速且准确的实体识别工具，非常适合于生产环境和大规模数据处理。  \n",
    "根据你的需要选择合适的工具。如果你的项目需要高准确度和高性能的实体识别，推荐使用spaCy。如果你在学习NLP基础或进行教学研究，NLTK也是一个不错的选择。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e2ddd",
   "metadata": {},
   "source": [
    "### 6. 词向量化\n",
    "将文本转换为向量表示是深度学习中的一个重要步骤，词向量化是将分词后的词语转换为向量表示的过程。因为模型是没法直接处理文本的，必须把文本用向量进行数学表示，才能在模型中参与各种计算。  \n",
    "常见的词向量化算法有词袋模型（Bag-of-words, BoW）、TDF-IDF、词嵌入算法等。\n",
    "\n",
    "**词嵌入（Word Embeddings）**   \n",
    "词嵌入是当前最流行的词向量化技术，它产生的向量捕获了丰富的语义和语法属性。  \n",
    "Word2Vec：通过神经网络模型从大量文本中学习词的向量表示。提供两种架构：CBOW（Continuous Bag of Words）和Skip-gram。  \n",
    "GloVe（Global Vectors for Word Representation）：通过矩阵分解技术在全局词-词共现统计上训练词向量。  \n",
    "FastText：类似于Word2Vec，但它不仅学习词的向量，还学习词根的向量，适用于处理形态丰富的语言。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f529769",
   "metadata": {},
   "source": [
    "TF-IDF（词频-逆文档频率）是一种常用的文本特征提取方法，它用于衡量单词在文档集合中的重要性。TF-IDF 是由两部分组成的：词频（TF）和逆文档频率（IDF）。词频（TF）表示单词在文档中出现的频率。它通常通过将单词在文档中出现的次数除以文档的总单词数来计算。词频越高，说明单词在文档中越重要。逆文档频率（IDF）表示单词在文档集合中的普遍重要性。它通常通过将文档集合的总文档数除以包含该单词的文档数，然后取对数来计算。逆文档频率越高，说明单词在文档集合中越重要。TF-IDF 值是通过将词频和逆文档频率相乘来计算的。TF-IDF 值越高，说明单词在文档中越重要。下面是直接使用sklearn来进行TDF-IDF向量化的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc53f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.94215562 0.33517574]\n",
      " [0.6316672  0.6316672  0.         0.44943642]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 准备数据\n",
    "corpus = ['赵六 王五 王五', '张三 赵六 李四']\n",
    "\n",
    "# 创建 TfidfVectorizer 对象\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 计算 TF-IDF 值\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 输出结果\n",
    "print(tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a56cdf",
   "metadata": {},
   "source": [
    "**使用Gensim训练Word2Vec模型** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e00842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00964853  0.00732483  0.00126166 -0.00340524 -0.00045066  0.00042262\n",
      " -0.00640921  0.00574516  0.0023714   0.00377794 -0.00725502  0.00852629\n",
      "  0.00050825 -0.00020393 -0.00907161  0.00404876  0.00676344  0.00735509\n",
      " -0.00641753 -0.00785606 -0.00552371 -0.0006029  -0.00833798 -0.00824012\n",
      " -0.00191757  0.00113929 -0.00950655 -0.00373293  0.00064478  0.00680997\n",
      "  0.00173499 -0.00063238 -0.00748139 -0.00674209 -0.00069443  0.00746758\n",
      "  0.00544287 -0.00148269  0.00117231 -0.00960578 -0.0013845  -0.00462622\n",
      "  0.00580931 -0.00233801 -0.00476407 -0.00947511 -0.00120031 -0.00719777\n",
      " -0.00168522 -0.00406982 -0.00237414 -0.00324834 -0.00815647 -0.00124841\n",
      "  0.00169094 -0.00404764 -0.00764145 -0.00358664 -0.00904713 -0.00075842\n",
      "  0.005883   -0.00296135  0.00316176  0.00499572  0.00846843  0.00562142\n",
      "  0.00950855 -0.00964668 -0.00796267 -0.00675816 -0.00746771 -0.00796362\n",
      " -0.00778817 -0.00294263  0.00139616 -0.00287479 -0.00881625  0.00498586\n",
      "  0.00090089  0.0045904   0.00719534  0.00764829 -0.00080465  0.00365996\n",
      " -0.00512358  0.00191068  0.00453827  0.00988666 -0.00318749  0.00283933\n",
      " -0.00573635 -0.00221057  0.0081251  -0.00390466 -0.00118858 -0.00928497\n",
      " -0.00947678  0.00888277 -0.00570292  0.00505279]\n",
      "[ 1.3283076e-03  6.5468010e-03  9.9907992e-03  9.0709664e-03\n",
      " -8.0169104e-03  6.4940928e-03 -5.7146791e-03 -9.7146770e-04\n",
      "  4.8681791e-04  6.5804003e-03  4.4747414e-03  4.6036993e-03\n",
      "  9.4911605e-03  3.8306604e-04 -6.0301782e-03 -6.3320380e-03\n",
      "  6.4333649e-03 -5.2470849e-03 -2.8569072e-03  4.0721931e-03\n",
      " -2.2875736e-03 -6.0184458e-03 -2.3220042e-03  1.2089296e-03\n",
      "  2.1770643e-03  6.0871099e-03 -5.2134269e-03  3.0767408e-03\n",
      "  7.2382330e-03  2.1918563e-03  5.3999871e-03 -4.8535997e-03\n",
      "  6.1518452e-03 -7.6010865e-03  3.4907910e-03 -9.3200095e-03\n",
      " -2.6003225e-03 -9.0693301e-03 -1.5946804e-03 -5.3611854e-03\n",
      " -3.9404901e-03  1.1593910e-03  2.7979175e-03 -1.5268771e-03\n",
      " -8.1705973e-03 -5.9287106e-03  8.1615103e-04 -3.9487956e-03\n",
      " -9.4321799e-03 -7.7161670e-04  6.6389702e-03  5.9767659e-03\n",
      " -9.9190306e-03  3.1158119e-03 -5.9924931e-03 -9.1771791e-03\n",
      "  1.7525101e-04 -3.6567918e-04 -6.9729281e-03 -6.2866816e-03\n",
      " -2.4309829e-03  7.0987944e-03 -7.5408919e-03  7.6958118e-03\n",
      " -4.8202634e-04  1.0972836e-03  9.4805909e-03  4.7269859e-03\n",
      " -3.5726081e-03  3.7344005e-03  3.5193469e-03  6.3400627e-03\n",
      "  6.3753352e-05 -4.4239219e-03  1.3202898e-03 -5.4123276e-03\n",
      "  1.4137544e-03  4.9282927e-03  5.1480792e-03  9.1823125e-03\n",
      " -7.5201569e-03 -5.4034088e-03  6.4727613e-03  1.3570562e-03\n",
      " -6.6166455e-03  8.9044136e-04  2.6795338e-03 -2.5299722e-03\n",
      " -4.9659298e-03  5.0129155e-03  9.6160341e-03 -7.3663415e-03\n",
      " -1.1817156e-04 -2.5643397e-03 -6.3587008e-03 -1.3771182e-03\n",
      " -5.2463943e-03  9.0614920e-03 -5.7949424e-03  3.6871445e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 示例文本\n",
    "sentences = [\"Apple is looking at buying U.K. startup for $1 billion\",\n",
    "             \"San Francisco considers banning sidewalk delivery robots\",\n",
    "             \"London is a big city in the United Kingdom\"]\n",
    "\n",
    "# 分词\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# 训练Word2Vec模型\n",
    "model = Word2Vec(tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 获取单词的向量\n",
    "vector = model.wv['apple']\n",
    "print(vector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f57f79",
   "metadata": {},
   "source": [
    "## 总结\n",
    "上面总结了NLP处理过程中可能进行的一些步骤和使用的技术。在真正的项目中，需要根据项目的需求和实际情况选择合适的处理方法。  \n",
    "下面我们通过真实的处理例子来介绍一下NLP有哪些实际应用。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
